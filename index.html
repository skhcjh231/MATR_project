<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Online Temporal Action Localization with Memory-Augmented Transformer">
  <meta property="og:title" content="Online Temporal Action Localization with Memroy-Augmented Transformer"/>
  <meta property="og:description" content="Youngkil Song et al."/>
  <meta property="og:url" content="https://skhcjh231.github.io/MATR"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/ontal_teaser.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Online Temporal Action Localization with Memory-Augmented Transformer">
  <meta name="twitter:description" content="Youngkil Song et al.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/ontal_teaser.png">
  <meta name="twitter:card" content="static/images/ontal_teaser.png">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Temporal Action Localization & Online Video Understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Online Temporal Action Localization with Memory-Augmented Transformer</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Online Temporal Action Localization with Memory-Augmented Transformer</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://skhcjh231.github.io" target="_blank">Youngkil Song*</a>,</span>
                <span class="author-block">
                  <a href="https://dk-kim.github.io" target="_blank">Dongkeun Kim*</a>,</span>
                  <span class="author-block">
                    <a href="https://cvlab.postech.ac.kr/~mcho" target="_blank">Minsu Cho</a>,</span>
                    <span class="author-block">
                      <a href="https://suhakwak.github.io/" target="_blank">Suha Kwak</a>,</span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Pohang University of Science and Technology (POSTECH), South Korea<br>ECCV 2024<br>* means equal contribution</span>
<!--                     <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/skhcjh231/MATR_codebase" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/combined_demo2.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <div style="margin: 0% 0% 0% 10%; float: left">Detection output</div>
        <div style="margin: 0% 8% 0% 0%; float: right">Touch (an object)</div>
        <div style="margin: 0 auto; width: 400px;">Smoke</div> -->
        <!-- <br> -->
        <!-- Our model focuses on regions dedicated to each action class to classify an actor's action. -->
      <!-- </h2>
    </div>
  </div>
</section>
<section class="hero teaser">
  <div class="container">
    <div class="hero-body">
      <video poster="" id="tree2" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/combined_demo1.mp4"
        type="video/mp4" width="100%">
      </video>
      <h2 class="subtitle has-text-centered">
        <div style="margin: 0% 0% 0% 10%; float: left">Detection output</div>
        <div style="margin: 0% 10% 0% 0%; float: right">Listen to (a person)</div>
        <div style="margin: 0 auto; width: 400px;">Talk to (e.g., self, a person, a group)</div>
        <br>
        Our model focuses on regions dedicated to each action class to classify an actor's action.
      </h2>
    </div>
  </div>
</section>  -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Online temporal action localization (On-TAL) is the task of identifying multiple action instances given a streaming video. Since existing methods take as input only a video segment of fixed size per iteration, they are limited in considering long-term context and require tuning the segment size carefully. To overcome these limitations, we propose memory-augmented transformer (MATR). MATR utilizes the memory queue that selectively preserves the past segment features, allowing to leverage long-term context for inference. We also propose a novel action localization method that observes the current input segment to predict the end time of the ongoing action and accesses the memory queue to estimate the start time of the action. Our method outperformed existing methods on two datasets, THUMOS14 and MUSES, surpassing not only TAL methods in the online setting but also some offline TAL methods.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Motivation -->
<!-- <section class="section hero is-small">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Why does classification matter in video action detection?</h2>
        <div class="hero-body">
          <img src="static/images/motivation1.png" width="60%" alt="motivation1" vspace="20"/>
          <div class="content has-text-justified">
            <p>
              Video action detection (VAD) inherently holds difficulties of accurately classifying an action rather than localizing, since the instances appearing in datasets are always human.
              The graph above shows how important the classification is; providing GT class labels improves the performance way more than providing GT box labels does.
            </p>
          </div>
          <img src="static/images/motivation2.png" width="100%" alt="motivation2" vspace="20" />
          <div class="content has-text-justified">
            <p>
              We discovered that previous state-of-the-art methods often failed to capture the correct context that corresponds to the performed actions. They particularly avoided browsing outside the actor region no matter what actions were being performed.
              It is somehow a natural behavior, since most models utilize a single attention map for each actor and gather semantics that are useful for classification,
              which drives a model to prioritize capturing commonly-shared semantics among classes: <i>the actor</i>.
              Intrigued by such observation, we propose to generate an attention map for each class, so that the model could explicitly focus on class-specific context rather than utilizing an actor-biased, ambiguous classification attention map.
            </p>
          </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End Motivation -->


<!-- Model Architecture -->
<section class="section hero is-small">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Model Architecture</h2>
        <div class="hero-body">
          <img src="static/images/overall_framework.png" width="100%" alt="architecture" vspace="20"/>
          <div class="content has-text-justified">
            <p>
              MATR consists of four parts: feature extractor, memory-augmented video encoder, instance decoding module, and prediction heads.
              The current segment of the input streaming video is given as a unit input, and its frame-level features,
              referred to as segment features, are extracted by a video backbone network and a linear projection layer.
              The segment features are then fed to the memory-augmented video encoder, which encodes temporal context between frames in the current
              segment and stores the segment features into the memory. The instance decoding module localizes action instances via two Transformer decoders:
              the end decoder and the start decoder. Specifically, the end decoder references the encoded segment features to locate the action end around the current time,
              and then the start decoder refers to the memory queue to find the action start based on the past information stored in the memory queue.
              Queries for each instance consist of a class query for action classification and a boundary query for action localization.
              The outputs of the instance decoding module are used as inputs to the prediction heads, which consist of end prediction head, start
              prediction head, and action classification head. The entire model is trained in an end-to-end manner.
            </p>
          </div>
      </div>
    </div>
  </div>
</section>
<!-- End Model Architecture -->

<!-- Experiments -->
<section class="section hero is-small">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
        <div class="hero-body">
          <img src="static/images/MATR_experiment_results.png" width="100%" alt="main table" vspace="20" />
          <div class="content has-text-justified">
            <p>
              Our method outperforms all previous On-TAL methods in both benchmarks by a substantial margin: 4.9%p of average mAP in THUMOS14 and 0.7%p of average mAP in MUSES.
            </p>
          </div>
          <img src="static/images/MATR_ablation.png" width="80%" alt="efficiency" vspace="20" />
          <div class="content has-text-justified">
            <p>
              We conduct an ablation study to verify the effectiveness of each component of our model. The results show that all design choices of the proposed components are important for action instance localization.
              Adddtionally, as shown in Table 3, employing the memory quque enhances the performance compared to the without memory case, and generally the performance improves as the memory queue size increases.
            </p>
          </div>
      </div>
    </div>
  </div>
</section>
<!-- End Experiments -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Qualitative results</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item", width="100%">
        <!-- Your image here -->
        <img src="static/images/matr_qual1.png" width="100%" alt="MY ALT TEXT"/>

      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/matr_qual2.png" width="100%" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/matr_qual3.png" width="100%" alt="MY ALT TEXT"/>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Other detection results</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video0">
          <video poster="" id="video0" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/ava_mp4/combined_ava.mp4" type="video/mp4" width="80%" height="100">
          </video>
        </div>
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/ucf_mp4/new_combined_ucf1.mp4" type="video/mp4" width="100%" height="100">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/ucf_mp4/new_combined_ucf2.mp4" type="video/mp4" width="100%" height="100">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/jhmdb_mp4/new_combined_jhmdb1.mp4" type="video/mp4" width="100%" height="100">
          </video>
        </div>
        <div class="item item-video4">
          <video poster="" id="video3" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/jhmdb_mp4/new_combined_jhmdb2.mp4" type="video/mp4" width="100%" height="100">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->




<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video presentation will be uploaded soon</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <!-- <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
          <!-- </div> -->
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster will be uploaded soon</h2>

      <!-- <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe> -->
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Will be updated soon</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
